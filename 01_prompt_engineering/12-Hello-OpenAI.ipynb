{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec17c3e-45f8-471c-ae07-d537384883f5",
   "metadata": {},
   "source": [
    "![nvidia](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4772d59",
   "metadata": {},
   "source": [
    "# Hello World with OpenAI Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df30528-2fcd-49b1-8291-2036e192fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from videos.walkthroughs import walkthrough_12 as walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8a50c-b1ea-47af-a975-2f067d7adcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "walkthrough()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013522",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to interact with the OpenAI API to generate text completions using the Llama 3.1 8b model. This introductory section will help you understand the basics of setting up and using the OpenAI library for interacting with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a70fb-0429-4036-82ce-a55c4262561a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08054f2",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a023bc7a-47b5-4508-957c-f3354c9fb363",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "\n",
    "- Understand how to set up and use the OpenAI library.\n",
    "- Generate text completions using the Llama 3.1 8b instruct model.\n",
    "- Learn to interpret and utilize the API response.\n",
    "- Understand the importance of using *chat* completion endpoints with chat models like Llama 3.1 8b instruct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b0fab-b9e3-4de9-bc46-5f31ab9ea623",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327550d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9128a04-4ba5-4762-a277-3e614725214b",
   "metadata": {},
   "source": [
    "Here we import the `OpenAI` library, which will enable us to interact with the NVIDIA API Catalog. The API Catalog exposes an OpenAI-compatible API, making it easy to work with using familiar tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75febe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a291cd0-5701-41dc-b3a4-229bce728f10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09eea9-c0d0-4962-a7d3-1d3f51b8573b",
   "metadata": {},
   "source": [
    "To start using the API, we need to set up the OpenAI client. This involves configuring the base URL and providing an API key.\n",
    "\n",
    "In this course environment, the connection details have been pre-configured as environment variables, so we can simply retrieve them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfe47a-1662-48f2-a9b0-57c224b1987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = os.getenv(\"NVIDIA_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc5680-8e7c-4962-a33d-aa8cc5c60cb0",
   "metadata": {},
   "source": [
    "The API key is also provided via environment variable. While you could use your own API key from [build.nvidia.com](https://build.nvidia.com/explore/discover), we've pre-configured one for this course:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49d2c0-9279-4172-b441-a2d4df435b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"NVIDIA_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a3cee-0beb-47b6-8442-3cebe57fe898",
   "metadata": {},
   "source": [
    "With a `base_url` and `api_key` we can now instantiate an OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=base_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce48a0-1348-4fd9-8bb1-cb3c05e3215a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca148f-e7db-4756-893e-6f2a2021b38d",
   "metadata": {},
   "source": [
    "## Observing Available Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48326ea-44b3-4b55-b625-844d98aec771",
   "metadata": {},
   "source": [
    "Now that we've created an OpenAI client, we can use it to observe the models available to us using a call to `client.models.list()`. The NVIDIA API Catalog provides access to many models, including the Llama 3.1 8B Instruct model we'll be using in this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f214e1-3a0a-4f80-aa13-6e44288a2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462496c6-5968-4835-ba60-d0ead7557104",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a122bd-19c3-4447-ac28-5276e43b9ca9",
   "metadata": {},
   "source": [
    "There's a lot of information here that we are not concerned with, but if we drill into the object a little we can see more clearly the model we have available through the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455ead1-bdc3-4ef1-91ee-74368495fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models.data[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977ce91-76c6-4d9f-b091-c74b86c047e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0f44e-e634-44c7-b903-8bab3c196452",
   "metadata": {},
   "source": [
    "## Making a Simple Chat Completion Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb9743-ba83-4d62-a776-feeea2bda8fd",
   "metadata": {},
   "source": [
    "With the `client` instance now created, we can make a simple request to generate chat completions by using the `client.chat.completions.create` method which expects a `model` to use for the completion, as well as a list of `messages` to send to the model. We will be discussing the details of the `messages` list in more detail below, but for now we will pass in a simple single message containing a prompt from the user (you) asking for a fun fact about space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8eb4e-b281-41fe-888e-2285d0669a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "prompt = 'Tell me a short fun fact about space.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f29d63-84bc-4a75-839f-f328507bfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b16139-d14f-46b6-9fc3-6a5f03c2a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d62be-d6c2-4cb8-a1f8-cfad5960e2b0",
   "metadata": {},
   "source": [
    "There's a fair amount of information provided in the API response, but the part we are most interested in is the response from the model.\n",
    "\n",
    "Here we parse just the model's generated response out of the full API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e152d-a72e-4489-be51-6b495ecd139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6ad10-476f-4e5e-bff0-0bdb01c9dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f086f-7f1f-41e7-8189-3184b521872e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b8732-cabd-42a9-b826-99d73c177037",
   "metadata": {},
   "source": [
    "## Exercise: Create Your First Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82b4bd-fa06-4d16-a606-ce78daea60fc",
   "metadata": {},
   "source": [
    "Use our existing OpenAI API `client` to generate and print a response from the Llama 3.1 8b model to a prompt of your choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5decfa36-bbf4-4cb8-9277-107f5c77ac67",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737761e-6b06-4895-b2a3-e90220a06533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "044bf0a3-b6ad-455d-a95e-a0cc9763225b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8bf54f-b631-45a1-b89c-c555f740e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What is the OpenAI API?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6cb76-7f78-47bf-ad1d-be363ee13727",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27550c33-309d-497d-b014-5806c942d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b0031-8f17-4fda-884d-8116cdd45db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a434ae7-0212-4dd7-95e3-09028e84412d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e582e-1dbf-411b-8611-46bfc681fc4d",
   "metadata": {},
   "source": [
    "## Understanding Completion and Chat Completion Endpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d468d177-3c90-46dc-9aa3-5b2b685c7619",
   "metadata": {},
   "source": [
    "We have been working with the `chat.completions` endpoint, but when working with the OpenAI API, you also have the option to use the `completions` endpoint. Understanding the differences between these endpoints is crucial, as they handle prompts and generate responses differently, even for a single prompt.\n",
    "\n",
    "The `chat.completions` endpoint is designed to handle multi-turn conversations, keeping track of the context provided by previous messages. It generates more concise, focused responses by anticipating a back-and-forth interaction, even if only a single prompt is provided.\n",
    "\n",
    "The `completions` endpoint is designed for generating a response to a single prompt without maintaining conversational context. It aims to complete the prompt that was given to it, rather than respond to it conversationally.\n",
    "\n",
    "The main takeaway is that when working with \"chat\" or \"instruction\" models (like the llama-3.1-8b-instruct model you are working with today), use `chat.completions` and not `completions`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69074d67-d351-40a8-9c68-5c5f5edadd48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e117ca2-b3de-44fe-b2c6-79c651838ce7",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bbb6e-2948-4812-b902-e2bb9d3386ab",
   "metadata": {},
   "source": [
    "By completing this notebook, you should now have a basic understanding of how to use the OpenAI library to generate chat completions, and parse out the model response. This foundation will prepare you for more advanced topics and techniques in prompt engineering.\n",
    "\n",
    "In the next notebook, we will explore how to use LangChain to interact with language models, which will provide more flexibility and advanced capabilities for managing and generating text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}