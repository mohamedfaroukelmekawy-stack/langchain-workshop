{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nvidia](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA NIM and the API Catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videos.walkthroughs import walkthrough_11 as walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkthrough()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understand what NVIDIA Inference Microservices (NIMs) are\n",
    "- Know how we will utilize the NVIDIA API Catalog to conduct prompt engineering\n",
    "- Understand the benefits of local NIM deployment for production workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Inference Microservices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA NIM is a set of easy-to-use microservices designed for secure, reliable deployment of high performance AI model inference across the cloud, data center and workstations. Supporting a wide range of AI models, including open-source community and NVIDIA AI Foundation models, it ensures seamless, scalable AI inferencing, on premises or in the cloud, implementing industry standard APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NVIDIA API Catalog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Course Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we will use the NVIDIA API Catalog to access the [meta/llama-3.1-8b-instruct](https://build.nvidia.com/meta/llama-3_1-8b-instruct) model. The API Catalog provides a convenient way to experiment with models and develop prompt engineering skills without needing to manage GPU infrastructure.\n",
    "\n",
    "We've configured this environment with the necessary credentials to access the API, so you can focus on learning prompt engineering techniques rather than setup details.\n",
    "\n",
    "Using the API Catalog is an excellent way to:\n",
    "- **Prototype quickly**: Start experimenting with models immediately without any infrastructure setup\n",
    "- **Learn and iterate**: Develop your prompt engineering skills with fast feedback loops\n",
    "- **Explore models**: Try different models to find the best fit for your use case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Course Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the API Catalog is great for learning and prototyping, you may eventually want to deploy NIMs locally or in your own infrastructure. NIM microservices are packaged as container images on a per model/model family basis, and can be deployed on NVIDIA GPUs with sufficient memory.\n",
    "\n",
    "Local NIM deployments offer several benefits for production workloads:\n",
    "\n",
    "- **Cost at Scale**: API-hosted LLMs can become expensive for large-scale or high-volume needs. Local deployments offer a more cost-effective solution for production workloads, as you can scale by adding computing resources or distributing across multiple machines.\n",
    "- **Data Privacy**: Keep sensitive data within your own infrastructure rather than sending it to external APIs.\n",
    "- **Customization and Control**: Running a model locally gives you full control over your AI applications, including the ability to fine-tune models and customize the serving infrastructure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NIMDeploymentLifecycle](images/NIM_Deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM NIM microservices have a variety of benefits, a few of which we'll touch on here.\n",
    "\n",
    "- **Speed**: LLM NIM microservices are supported with pre-generated optimized engines for a diverse range of cutting edge LLM architectures, allowing for low latency when making inference.\n",
    "- **Scalable Deployment**: API-hosted LLMs can be expensive for large-scale or high-volume needs, but local deployments offer a more cost-effective solution. By investing in the initial setup, you can scale locally hosted models easily by adding computing resources or distributing them across multiple machines.\n",
    "- **Ownership**: Once set up, running a model locally gives you ownership of the customization and full control of your intellectual property and AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you were introduced to NVIDIA NIM microservices and the API Catalog. The API Catalog makes it easy to get started with powerful language models, and when you're ready for production, you can deploy NIMs in your own infrastructure.\n",
    "\n",
    "Now let's proceed to the next notebook where you will begin interacting with the Llama-3.1 8b instruct model through the API Catalog.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}